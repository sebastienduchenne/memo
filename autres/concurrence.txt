Concurrence
parallélisme
mémoire partagée
mémoire répartie
non-déterminisme
synchronisation
communication
section critique
exclusion mutuelle
algorithme de Dekker
algorithme de Peterson
sémaphore

----------------------------------------------

*** Pourquoi la concurrence

séquentialité = une instruction exécutée après l’autre. Le programme est déterministe
pallalélisme = plusieurs instructions s’exécutent en même temps dans des pcs différents. Cela introduit de la concurrence. Le programme devient non-déterministe, cad que, qu’il se termine ou pas, il produira des résultats différents
concurrence des pcs = exécution en parallèle de prog sur des machines
prog concurrente = paradigme tenant compte dans un prog de l’existence de plusieurs piles sémantiques
application parallèle = ensemble des processus interagissant sur une mémoire commune ou communiquant via un média

or la concurrence peut poser des problèmes et des contraintes
zone critique : si pas d’exclusion alors possible résultat incohérent, donc pour avoir un résultat cohérent il faut accès à une ressource avec des contraintes à l’exécution des pcs tel que mutex, sémaphore
non déterminisme
débogage plus difficile
gestion mémoire
liveness = famine
livelock = cycle non productif / verrou actif
deadlock = chacun attend l’autre
attente active = boucle sur un l’état d’un objet. MAUVAIS car consomme de la ressource CPU pour rien


*** Modèles de parallélisme

-système à mémoire partagée
    = plusieurs processus séquentiels interagissant sur une mémoire commune
        communication implicite
        synchronisation explicite en utilisant l’exclusion mutuelle et l’attente sur condition
        utilisée quand on se sert de manière concurrente d’une ressource commune
    système à mémoire répartie
    = chaque pcs a sa mémoire privée.  Ils doivent communiquer pour tranférer de l’information à travers un média assurant ces transferts. Des programmes appelés protocoles se chargent de gérer le flux d’informations
        communication explicite
        synchronisation implicite
        protocoles qui permettent la synchronisation


-Autres caractéristiques
    synchronisation
        bloquer l’exécution de certains pcs à des points précis de leur flux d’exécution de manière que tout les pcs se rejoignent à des étapes relais données
        = se synchroniser (ex : attendre une condition)
    communication = envoi et réception d’informations entre pcs


-Modèles de communication
    un à un
    un à tous
    tous à tous


-Types de communication
    synchrone = transfert d’informations possible que lors d’une synchronisation globale des pcs. Un pcs envoie un message puis attend la réponse de l’autre pcs pour travailler ou envoyer un autre message. On dit que l’émission et la réception peuvent être bloquante
    asynchrone = pas de synchronisation des pcs. Le medium peut stocker des messages en vue de leur acheminement futur. L’émission est non bloquante
    évanescent = le message n’est reçu que par les pcs prêt à recevoir et perdu pour les autres car le medium ne le stocke pas


*** Section critique et exclusion mutuelle

section critique = ressource qui ne doit être utilisée que par un pcs au plus
les pcs doivent s’exclure mutuellement de la section critique

-Algorithme d’exclusion mutuelle
    Dekker
        une variable globale que chaque pcs peut consulter et changer dans la section critique
        un pcs voulant entrer dans la SC met à 0 sa case, puis il regarde si l’autre est dans le même état
        si non, il entre dans la SC
        si oui, il consulte l’arbitre qui indique à qui est le tour, mais il ne peut être modifié que dans la SC par le pcs l’occupant en lui indiquant l’autre pcs
    Peterson
        une variable globale que chaque pcs peut consulter et changer dans la section critique
        pc1 veux entrer dans la SC met à 0 sa case
        pc1 donne la priorité à pc2. pc1 attend que pc2 signale son refus ou qu’il redonne à pc1 la priorité


-Sémaphore
    variable qui sert à partager des ressources (protéger une section critique) et à la synchronisation de pcs. Elle ne prend qu’une valeur entière positive ou nulle
    initialisé au nombre de ressources
    Si il est positif, il représente le nombre de ressources libre. Si il est négatif, sa valeur absolue représente le nombre de pcs en attente
    2 opérations indivisibles peuvent être effectuées dessus
        P / tester / “Puis-je ?” : sem--, si sem > 0 alors pcs utilise une ressource, si <= 0 alors pcs attend jusqu’a ce qu’un ressource soit disponible
        V / incrémenter / “Vas-y” : sem++, réveiller aléatoirement un pcs qui attend
    utilisation : le pcs fait WAIT, utilise la ressource, puis SIGNAL
    usage : 
        diner des philosophes (interblocage, famine)
        exclusion mutuelle si init à 1 = sémaphore binaire
        producteur-consommateur
    limite : 
        interblocage
        inversion de priorité


*** Le diner des philosophes

plusieurs pcs ont besoin en même temps de 2 ressources
3 règles
    1
    2
    3
solutions
    sémaphore
    si pair
    Chandy/Misra

-------------------------------------------------------------------------------------

OS
    prof : logiciel qui dirige l’utilisation du matériel (CPU, I/O, mémoire) par les autres logiciels
    wiki : ensemble de programmes qui dirige l’utilisation des ressources d’un ordinateur par des logiciels applicatifs
    intermédiaire entre le matériel informatique et les logiciels
ordonnanceur / scheduler
    prof : partie de l’OS qui décide du passage des pcs de l’état actif (utilisation du CPU) à l’état passif, quel prog tourne sur quel proc
    composant du noyau du OS choisissant l’ordre d’exécution des pcs sur les proc
préemption = suspension de l’exécution d’un pcs sur une unité de calcul (avant terminaison) pour donner la main à un autre pcs qui lui même est en attente. Quand on passe d’un pcs à un autre il y a changement de contexte (context switch), ce qui prend du temps. Cela donne l’illusion du parallélisme
processus = représentation abstraite de l’exécution d’un programme
thread = fil d’exécution, pcs léger



                    thread          processus
mémoire             partagée        cloisonnée
pid                 unique          Propre pid
environnement       partagé         cloisonné
Context-switch      Peu couteux     couteux
Pile d’exécution    cloisonnée      cloisonnée


*** Bon ordonnanceur
vivacité
sureté
équitable
efficace


*** Problème du producteur-consommateur
il s’agit de partager entre 2 taches, un producteur et un consommateur, une file
exemple de synchronisation de ressources
Le producteur génère un élément de données, l'enfile sur la file et recommence ; simultanément, le consommateur retire les données de file
